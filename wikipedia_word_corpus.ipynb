{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wikipedia word corpus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYdn1woOS1n",
        "outputId": "74730b00-1939-4ed7-f5b7-3b00ff134f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install bs4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH8XHrgKjxOA",
        "outputId": "be409acf-f753-4522-f718-c7a3c03d9437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOiKSA_fj3CI",
        "outputId": "ee83b4dc-2e60-4591-deb1-72794475b858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "import bs4\n",
        "import requests\n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "!pip install Unidecode\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 2.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGuWMF_q2Kvx"
      },
      "source": [
        "We'll be starting the collection of our corpus from Wikipedia webpage. To find the medical data we can find numerous links and words on the word 'Medical' on wikipedia. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yu5oL9Xj-HV"
      },
      "source": [
        "req = requests.get('https://en.wikipedia.org/wiki/Medicine') \n",
        "soup = bs4.BeautifulSoup(req.text,'lxml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWWbakd9kp1E"
      },
      "source": [
        "# this code generates a list with all the webscrapped links\n",
        "count = 0\n",
        "linklist = [all]\n",
        "for link in soup.find_all('a',href = True):\n",
        "  if link['href'][0] == '/':\n",
        "    link['href'] = 'https://en.wikipedia.org/'+link['href']\n",
        "  if link['href'][0] != '#':\n",
        "    #print(link['href'])\n",
        "    count+=1\n",
        "    linklist.append(link['href'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0u-hZNHo0Fe",
        "outputId": "9556b6bc-f4ab-4cc9-b264-e2bc737c3bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('The total number of sites are:',count) #on webscrapping we found out 1373 sites related to Medical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of sites are: 1373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzfye4khwHby",
        "outputId": "990e5289-05da-4598-95a5-9f067aabaac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(linklist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZBSw8kCY37T"
      },
      "source": [
        "# code for preprocessing and cleaning the text, we'll be using it later for getting the output medical words\n",
        "def preprocessing(text):\n",
        "  text = unidecode(text)\n",
        "  engwords = set(nltk.corpus.words.words())\n",
        "  text = \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
        "         if w.lower() in engwords or not w.isalpha())\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  text = text.replace(\"\\t\",\" \")\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>|./?@#$%^&*_~1234567890'''\n",
        "  for x in text:\n",
        "    if x in punctuations: \n",
        "      text = text.replace(x, \" \")\n",
        "  corpus = text.lower().split(\" \")\n",
        "  stop_words = stopwords.words('english')+list(string.punctuation)\n",
        "  cleantext = []\n",
        "  for i in corpus:\n",
        "    i = i.replace(\"\\n\",\" \")\n",
        "    for x in i: \n",
        "        if x in punctuations: \n",
        "            i = i.replace(x, \"\") \n",
        "    cleantext.append(i)\n",
        "  morecleantext = []\n",
        "  lem = WordNetLemmatizer()\n",
        "  for w in cleantext:\n",
        "    if w not in stop_words:\n",
        "      w = lem.lemmatize(w)\n",
        "      morecleantext.append(w)\n",
        "  morecleantext = list(dict.fromkeys(morecleantext))\n",
        "  morecleantext = ' '.join(morecleantext)\n",
        "  return morecleantext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnP_Gs2XzRLJ"
      },
      "source": [
        "# creating a text document with all the medical data from the links which is then preprocessed from the above code.\n",
        "with open('data.txt', 'w') as f:\n",
        "  for i in linklist:\n",
        "    try:\n",
        "      req = requests.get(i)\n",
        "      soup = bs4.BeautifulSoup(req.text,'lxml')\n",
        "      [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
        "      visible_text = soup.getText()\n",
        "      soup = preprocessing(visible_text)\n",
        "      f.writelines(soup)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_xZMlzHv_w7",
        "outputId": "c5655c1f-ac01-4c9b-eb92-1de3ac3b6571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKfuCZO9DnW"
      },
      "source": [
        "#this code gives us a mapping of the word and the word vector\n",
        "def read_glove_vecs(glove_file):\n",
        "      with open(glove_file, 'r') as f:\n",
        "          words = set()         # ensures unique values\n",
        "          word_to_vec_map = {}  # this will be a dictionary mapping words to their vectors\n",
        "          for line in f:\n",
        "              line = line.strip().split()\n",
        "              curr_word = line[0]\n",
        "              words.add(curr_word)\n",
        "              word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "      return word_to_vec_map,words\n",
        "word_to_vec_map,words = read_glove_vecs('/content/drive/My Drive/Copy of glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GfLXkpEnG8W",
        "outputId": "bf8af4c8-9e7a-4602-f519-da9cb0fb6da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('data.txt','r') as f:\n",
        "  text = f.read()\n",
        "  text = text.lower().split(\" \")\n",
        "  text = list(dict.fromkeys(text))\n",
        "  with open('/content/drive/My Drive/in_vocab1.txt','a+') as f:\n",
        "    for i in text:\n",
        "      if i in words:\n",
        "        f.writelines(i)\n",
        "        f.writelines(' ')\n",
        "  with open('/content/drive/My Drive/out_vocab1.txt','a+') as g:\n",
        "    for i in text:\n",
        "      if i not in words:\n",
        "        g.writelines(i) \n",
        "        g.writelines(' ')\n",
        "print(len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHyGhjA5_X1A"
      },
      "source": [
        "# saving words which are out of vocabulary in a seperate file  (words not in glove embedding)\n",
        "with open('/content/drive/My Drive/out_vocab1.txt','r') as g:\n",
        "  outvocab = g.read()\n",
        "  outvocab = outvocab.lower().split(\" \")\n",
        "#outvocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0LutGbgEJLN"
      },
      "source": [
        "# saving words which are in vocab in a seperate file (the medical words which are in glove embedding)\n",
        "with open('/content/drive/My Drive/in_vocab1.txt','r') as f:\n",
        "  invocab = f.read()\n",
        "  invocab = invocab.lower().split(\" \")\n",
        "#invocab"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}